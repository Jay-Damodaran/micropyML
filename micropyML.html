<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>micropyml — MicroPython ML Utilities</title>
    <style>
        body {
            font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
            line-height: 1.6;
            max-width: 900px;
            margin: 2rem auto;
            padding: 0 1rem;
            color: #222;
        }
        h1, h2 {
            color: #003366;
        }
        .section-divider {
            border: none;
            border-top: 5px solid #ddd;
            margin: 2rem 0 1.5rem 0;  /* space above / below */
        }
        code {
            background: #f5f5f5;
            padding: 2px 6px;
            border-radius: 4px;
            font-size: 0.95em;
        }
        .category + .function {
            border-top: none;
            padding-top: 0rem; 
            /*margin-bottom: 0.5rem;   reduce space under the heading */
        }
        .function {
            border-top: 5px solid #ddd;
            margin-top: 2.5rem;
            padding-top: 2rem;
        }
        .signature {
            font-family: monospace;
            font-size: 1.05em;
            margin-bottom: 0.5rem;
        }
        .section-title {
            font-weight: bold;
            margin-top: 1rem;
        }
        .category {
            font-weight: bold;
            color: #000;
            margin-top: 2rem;
            font-size: 1.2em;
        }
        a {
            text-decoration: none;   /* remove underline from anchor */
            color: #003366;
        }

        a code {
            text-decoration: underline;   /* underline only function name */
        }

        a:hover code {
            font-weight: bold;            /* optional: bold on hover */
        }
        .func-list {
            margin-left: 1.5rem;
        }
    </style>
</head>
<body>

<h1>micropyML</h1>
<p>
    <strong>micropyML</strong> is a lightweight embedded machine-learning utility module for MicroPython,
    built on top of <code>ulab.numpy.ndarray</code>. It provides common neural-network operations
    optimized for microcontrollers and resource-constrained environments.
</p>

<p>
    micropyML functions:
</p>
<ul>
    <li>Never modify ndarray inputs inplace</li>
    <li>Require a <code>ndarray</code> as the first argument for almost all functions</li>
    <li>Return a <code>ndarray</code> for almost all functions</li>
    <li>Do <strong>not</strong> accept keyword arguments</li>
    <li>Only work with arrays that are a maximum of 3 dimensions (for now)</li>
    <li>For user convenience, verify datatypes and dimensionality of arguments as function preconditions; however, no shape/length checks are performed as of now,
        so if array shapes/lengths are incompatible, unexpected behavior may occur i.e. Indexing Out of Bounds.
        <ul><li>For example, conv1d checks that all inputs are ndarrays and that x and kernel have 3-dims while bias has 1-dim. However, it
            doesn't verify that the Cin dimension of x matches that of kernel, or that the Cout dimension of kernel matches the length of bias.
        </li></ul>
    </li>
</ul>

<h2>Function Overview</h2>

<div class="category">Activations</div>
<div class="func-list">
    <p>
        <a href="#relu"><code>relu(x)</code></a><br>
        Rectified Linear Unit activation, element-wise.
    </p>
    <p>
        <a href="#softmax"><code>softmax(x)</code></a><br>
        Fixed-point softmax for integer input arrays.
    </p>
    <p>
        <a href="#confidence"><code>confidence(x)</code></a><br>
        Convert fixed-point softmax output to percent confidence.
    </p>
</div>

<div class="category">Layers</div>
<div class="func-list">
    <p>
        <a href="#dropout"><code>dropout(x, p=0.5)</code></a><br>
        Apply dropout regularization.
    </p>
    <p>
        <a href="#maxpool1d"><code>maxpool1d(x, k=2)</code></a><br>
        1D max pooling over a 3D input tensor.
    </p>
    <p>
        <a href="#conv1d"><code>conv1d(x, kernel, bias=<b>0</b>)</code></a><br>
        Floating-point 1D convolution layer.
    </p>
    <p>
        <a href="#qconvrelu1d"><code>qconvrelu1d(x, kernel, bias=<b>0</b>, quant_params=(1, 0, 0))</code></a><br>
        Quantized 1D convolution with ReLU activation fused in.
    </p>
</div>

<h2>Function Documentation</h2>

<div class="category">Activations</div>

<div class="function" id="relu">
    <h3>Relu</h3>
    <div class="signature">micropyML.relu(x)</div>

    <p>Apply the ReLU (Rectified Linear Unit) activation function element-wise.</p>

    <div class="section-title">Parameters</div>
    <ul>
        <li><code>x</code> (<code>ndarray</code>): Input array of any numeric dtype.</li>
    </ul>

    <div class="section-title">Returns</div>
    <ul>
        <li><code>ndarray</code>: Output array with the same shape and dtype as <code>x</code>,
            where negative values are clipped to zero.</li>
    </ul>
</div>

<div class="function" id="softmax">
    <h3>Softmax</h3>
    <div class="signature">micropyML.softmax(x)</div>

    <p>Compute the softmax activation for integer-valued input arrays.</p>

    <div class="section-title">Parameters</div>
    <ul>
        <li><code>x</code> (<code>ndarray</code>): array of shape (B, N) with dtype 8-bit uint/int.
            Softmax is computed along the last dimension (N). Only guaranteed to be safe from overflow 
            if N < 1024.</li>
    </ul>

    <div class="section-title">Returns</div>
    <ul>
        <li><code>tuple(tuple(int), ndarray)</code>: First element is tuple of size B that has indices of max probabilities (argmax).
            Second element is fixed-point Q7.7 output array with dtype <code>uint16</code> and shape (B, N).</li>
    </ul>

    <div class="section-title">Notes</div>
    <ul>
        <li>Maximum output value is <code>100 * 2<sup>7</sup></code>, representing probability 100%.</li>
        <li>Designed for integer-only inference pipelines.</li>
    </ul>
</div>

<div class="function" id="confidence">
    <h3>Confidence</h3>
    <div class="signature">micropyML.confidence(x)</div>

    <p>Convert fixed-point softmax output into confidence values suitable for display.</p>

    <div class="section-title">Parameters</div>
    <ul>
        <li><code>x</code> (<code>uint16 or ndarray</code>): Fixed-point softmax output for a single class or multiple classes.
            If x is an ndarray, it must be 1D and have dtype uint16.</li>
    </ul>

    <div class="section-title">Returns</div>
    <ul>
        <li><code>tuple(uint8) or tuple(tuple(uint8))</code>: If a single scalar uint16 is supplied, a 2 element tuple is returned. 
            First element is the integer part of the confidence (0-100). Second element is the fractional part of confidence (0-99).
            This makes it easy to use with python f strings. For example, print(f"{conf[0]}.{conf[1]}%") displays confidence as percentage.
            If a ndarray of uint16 is supplied, then a tuple is returned of the same length. Each entry in the returned tuple is a 2 element tuple
            with the same format mentioned above.</li>
    </ul>
</div>

<hr class="section-divider">
<div class="category">Layers</div>

<div class="function" id="dropout">
    <h3>Dropout</h3>
    <div class="signature">micropyML.dropout(x, p=0.5)</div>

    <p>Apply dropout regularization to an input array.</p>

    <div class="section-title">Parameters</div>
    <ul>
        <li><code>x</code> (<code>ndarray</code>): Input array of any numeric dtype.</li>
        <li><code>p</code> (<code>float</code>, Optional): Dropout probability (default 0.5).</li>
    </ul>

    <div class="section-title">Returns</div>
    <ul>
        <li><code>ndarray</code>: Output array with randomly zeroed elements. Same shape and dtype as <code>x</code>.</li>
    </ul>
</div>

<div class="function" id="maxpool1d">
    <h3>Maxpool1d</h3>
    <div class="signature">micropyML.maxpool1d(x, k=2)</div>

    <p>Perform 1D max pooling over a 3D input tensor.</p>

    <div class="section-title">Parameters</div>
    <ul>
        <li><code>x</code> (<code>ndarray</code>): Input array of shape <code>(B, C, N)</code>.</li>
        <li><code>k</code> (<code>int</code>, Optional): Kernel/Pool size as well as Stride(default 2).</li>
    </ul>

    <div class="section-title">Returns</div>
    <ul>
        <li><code>ndarray</code>: Pooled output array of shape (B, C, N//k) with dtype same as <code>x</code>.
        Since last dim is of size <code>N//k</code>, if N % k &ne; 0, the last element(s) are ignored when pooling.</li>
    </ul>
</div>

<div class="function" id="conv1d">
    <h3>Conv1d</h3>
    <div class="signature">micropyML.conv1d(x, kernel, bias=<b>0</b>)</div>

    <p>Perform a floating-point 1D convolution, meaning all inputs must be float. Function doesn't cast automatically.
        Stride and dilation are 1, and padding is 'same' for this implementation.</p>

    <div class="section-title">Parameters</div>
    <ul>
        <li><code>x</code>(<code>ndarray</code>): Shape <code>(B, Cin, N)</code></li>
        <li><code>kernel</code>(<code>ndarray</code>): Shape <code>(Cout, Cin, K)</code></li>
        <li><code>bias</code>(<code>ndarray</code>, Optional): Shape <code>(Cout)</code></li>
    </ul>

    <div class="section-title">Returns</div>
    <ul>
        <li><code>ndarray</code>: Output of shape <code>(B, Cout, N)</code> and dtype float</li>
    </ul>
</div>

<div class="function" id="qconvrelu1d">
    <h3>Quantized Fused ConvRelu1d</h3>
    <div class="signature">micropyML.qconvrelu1d(x, kernel, bias=<b>0</b>, quant_params=(1, 0, 0))</div>

    <p>Quantized 1D convolution with fused ReLU activation for integer-only inference. 
        Stride and dilation are 1, and padding is 'same' for this implementation.</p>

    <div class="section-title">Parameters</div>
    <ul>
        <li><code>x</code>(<code>ndarray</code>): Quantized input array of shape (B, Cin, N) and dtype (8- or 16-bit uint or int).</li>
        <li><code>kernel</code>(<code>ndarray</code>): Quantized kernel array of shape (Cout, Cin, K) and dtype (8- or 16-bit uint or int).</li>
        <li><code>bias</code>(<code>ndarray</code>, Optional): Bias array of shape (Cout,) and dtype int32.</li>
        <li><code>quant_params</code>(<code>tuple</code>, Optional): 3 integer tuple of quantization parameters
            <code>(multiplier, shift, output_zero_point)</code>. Uses the quantization scheme described by Google in 
            <a href="https://arxiv.org/pdf/1712.05877"><u>Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</u></a>.
            Multiplier is the final int32 result of multiplying the effective scale (<span><sup>S1 × S2</sup>&frasl;<sub>S3</sub></span>) 
            by 2<sup>shift + 31</sup>. Both the multiplier and shift are expected to be non-negative. 
            Output zero point corresponds to Z<sub>3</sub> in equation (4).</li>
    </ul>

    <div class="section-title">Returns</div>
    <ul>
        <li><code>ndarray</code>: Quantized output array of same dtype and shape as <code>x</code>.</li>
    </ul>
    <div class="section-title">Notes</div>
    <ul>
        <li><code>x</code> and <code>kernel</code> scales are wrapped into the multiplier, but if they have non-zero zero points, it is expected that
        the ndarrays will have this value subtracted before calling this function.</li>
    </ul>
</div>

</body>
</html>
